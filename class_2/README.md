This lecture will cover:

- Backpropagation (Single Input): Backprop is the fundamental algorithm used to train Neural Networks. We'll derive the gradient of the loss against various parameter matrices
- Backpropagation (Batched case): We'll extend the derivation above to include batched inputs
- Updating weights: Using the gradients calculated above to update the parameters of the network
- Example: Predicting the value of Sin(x)
- Minibatch Stochastic Gradient Descent: Averaging gradients over a sample of the input data 
- Implementing MSGD from scratch
- Pytorch Dataloader/Dataset classes
